{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fcf777d",
   "metadata": {},
   "source": [
    "You are evaluating a candidate sentiment model to replace a production baseline. Your goal is to determine whether this model should ship.\n",
    "\n",
    "‚ÄúShip‚Äù means: we would choose the candidate model over the baseline for deployment based on the evidence you collect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfc1c7",
   "metadata": {},
   "source": [
    "### Step 1 - Install the required dependencies, set up W&B and make sure the python version is 3.10 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1012c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q wandb datasets transformers evaluate tqdm emoji regex pandas pyarrow scikit-learn nbformat torch\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01639808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262b991",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f18b443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.2\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8e44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and config:\n",
    "import re, regex, emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# WANDB CONFIG\n",
    "PROJECT = \"mlip-lab4-slices-2026\"\n",
    "ENTITY = None\n",
    "RUN_NAME = \"baseline_vs_candidate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a6beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare\n",
    "MODELS = {\n",
    "    \"baseline_model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    \"candidate_model\":    \"LYTinn/finetuning-sentiment-model-tweet-gpt2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "434c2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label normalization for tweet_eval (0/1/2 -> string labels)\n",
    "ID2LABEL = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "# Many HF sentiment models output labels like LABEL_0 / LABEL_1 / LABEL_2\n",
    "HF_LABEL_MAP = {\"LABEL_0\": \"negative\", \"LABEL_1\": \"neutral\", \"LABEL_2\": \"positive\"}\n",
    "\n",
    "USE_HF_DATASET = True  # set False to use tweets.csv fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289dcac3",
   "metadata": {},
   "source": [
    "### Step 2 - Load a dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9575ae14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b13b760a1d496ab3d961cfcf80afd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b5728c2945466e9622954454614fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentiment/train-00000-of-00001.parquet:   0%|          | 0.00/3.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e55c5cd52a48f8982d30b27233e906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentiment/test-00000-of-00001.parquet:   0%|          | 0.00/901k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5074294456044a3e99b7e3f351cdf36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentiment/validation-00000-of-00001.parq(‚Ä¶):   0%|          | 0.00/167k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7156b27045e54e349183451edfbd7959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/45615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8eee4bc30ed4da48859dbc6783a8da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/12284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52eae47a22064d6a8ffe57c752bf3375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user @user what do these '1/2 naked pics' hav...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH: ‚ÄúI had a blue penis while I was this‚Äù [pla...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user That's coming, but I think the vic...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  @user @user what do these '1/2 naked pics' hav...  neutral\n",
       "1  OH: ‚ÄúI had a blue penis while I was this‚Äù [pla...  neutral\n",
       "2  @user @user That's coming, but I think the vic...  neutral"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if USE_HF_DATASET:\n",
    "    ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "    df = pd.DataFrame(ds[\"test\"]).head(500).copy()\n",
    "    df[\"label\"] = df[\"label\"].map(ID2LABEL)\n",
    "else:\n",
    "    df = pd.read_csv(\"tweets.csv\")\n",
    "    # Ensure it has 'text' and 'label' columns\n",
    "    df = df.rename(columns={c: c.strip() for c in df.columns})\n",
    "    assert {\"text\",\"label\"}.issubset(df.columns), \"tweets.csv must include text,label\"\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.lower()\n",
    "\n",
    "df = df[[\"text\",\"label\"]].dropna().reset_index(drop=True)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0546f1",
   "metadata": {},
   "source": [
    "### Step 3 - Define Failure-Relevant Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089146d4",
   "metadata": {},
   "source": [
    "#TODO:\n",
    "In this step, you will create **at least 5** metadata columns that help you slice and analyze model behavior in Weights & Biases (W&B).\n",
    "These metadata columns should **capture meaningful properties of the data or model behavior that may influence performance**. You can define them using:\n",
    "\n",
    "1. Value matching (e.g., tweets containing hashtags or mentions)\n",
    "2. Regex patterns (e.g., negation words, strong sentiment terms like love or hate)\n",
    "3. Heuristics (e.g., emoji count, all-caps text, tweet length buckets)\n",
    "\n",
    "Each metadata column should correspond to a potential hypothesis about when or why a model might succeed or fail.\n",
    "These columns will be propagated through inference and included in the final predictions_table logged to W&B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62499536",
   "metadata": {},
   "source": [
    "After inference, your W&B table (df_long) will contain:\n",
    "- The original tweet text\n",
    "- Ground-truth sentiment labels\n",
    "- Model predictions and confidence scores\n",
    "- All metadata columns you defined for slicing\n",
    "\n",
    "You will use these metadata fields in the W&B UI (via the ‚ûï Filter option) to:\n",
    "- Create slices of the data\n",
    "- Compare model behavior across slices\n",
    "- Identify patterns, weaknesses, or regressions that are not visible in overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fd5775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tj/f3l3_hgj4wng8cx7x4cwjzzc0000gn/T/ipykernel_30477/866497519.py:12: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[\"has_negation\"] = df[\"text\"].str.contains(r\"\\b(not|never|no)\\b\", regex=True)\n",
      "/var/folders/tj/f3l3_hgj4wng8cx7x4cwjzzc0000gn/T/ipykernel_30477/866497519.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[\"has_strong_sentiment\"] = df[\"text\"].str.contains(r\"\\b(love|hate|amazing|terrible|worst|best)\\b\", case=False, regex=True)\n",
      "/var/folders/tj/f3l3_hgj4wng8cx7x4cwjzzc0000gn/T/ipykernel_30477/866497519.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[\"has_sarcasm_indicators\"] = df[\"text\"].str.contains(r\"\\b(yeah right|sure|totally|obviously)\\b\", case=False, regex=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 3 ‚Äì Add slicing metadata (text-only)\n",
    "\n",
    "# TODO: add your own hypothesis-driven metadata here. \n",
    "# Here are examples of the kinds of metadata columns you can add & analyse.\n",
    "# Categories you can explore are: Linguistic, Emotional/semantic, Model-behavioral. Do not reuse the ones given below.\n",
    "def count_emojis(text: str) -> int:\n",
    "    return sum(ch in emoji.EMOJI_DATA for ch in str(text))\n",
    "\n",
    "df[\"emoji_count\"] = df[\"text\"].apply(count_emojis).astype(int)\n",
    "df[\"has_hashtag\"] = df[\"text\"].str.contains(r\"#\\w+\", regex=True)\n",
    "df[\"has_mention\"] = df[\"text\"].str.contains(r\"@\\w+\", regex=True)\n",
    "df[\"has_negation\"] = df[\"text\"].str.contains(r\"\\b(not|never|no)\\b\", regex=True)\n",
    "df[\"length_bucket\"] = pd.cut(\n",
    "    df[\"text\"].str.len(),\n",
    "    bins=[0, 50, 100, 200, 1000, 10_000],\n",
    "    labels=[\"0-50\", \"51-100\", \"101-200\", \"201-1000\", \"1001+\"],\n",
    "    include_lowest=True\n",
    ").astype(str)\n",
    "\n",
    "# Additional hypothesis-driven slices\n",
    "df[\"has_all_caps\"] = df[\"text\"].apply(lambda x: any(word.isupper() and len(word) > 2 for word in str(x).split()))\n",
    "df[\"has_question\"] = df[\"text\"].str.contains(r\"\\?\", regex=True)\n",
    "df[\"has_url\"] = df[\"text\"].str.contains(r\"http[s]?://|www\\.\", regex=True)\n",
    "df[\"has_strong_sentiment\"] = df[\"text\"].str.contains(r\"\\b(love|hate|amazing|terrible|worst|best)\\b\", case=False, regex=True)\n",
    "df[\"has_sarcasm_indicators\"] = df[\"text\"].str.contains(r\"\\b(yeah right|sure|totally|obviously)\\b\", case=False, regex=True)\n",
    "\n",
    "# Example slice definitions (you'll create more later)\n",
    "def get_slices(df_any: pd.DataFrame):\n",
    "    return {\n",
    "        \"emoji_gt3\": df_any[\"emoji_count\"] > 3,\n",
    "        \"has_negation\": df_any[\"has_negation\"] == True,\n",
    "        \"has_hashtag\": df_any[\"has_hashtag\"] == True,\n",
    "        \"has_all_caps\": df_any[\"has_all_caps\"] == True, # Added\n",
    "        \"has_question\": df_any[\"has_question\"] == True, # Added\n",
    "        \"has_strong_sentiment\": df_any[\"has_strong_sentiment\"] == True, # Added\n",
    "        \"short_tweets\": df_any[\"length_bucket\"] == \"0-50\", # Added\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac601ea",
   "metadata": {},
   "source": [
    "### Assumptions / Hypotheses for the *new* metadata columns\n",
    "These metadata columns are designed to create hypothesis-driven slices in W&B (to reveal failure modes beyond overall accuracy).\n",
    "\n",
    "- `has_all_caps` (ALL CAPS emphasis): Tweets with shouting/emphasis may be misread as stronger sentiment than intended, or may correlate with anger/excitement. Expect higher error/regression when emphasis contradicts literal wording.\n",
    "- `has_question` (contains `?`): Questions are often informational/neutral, but rhetorical questions can imply sentiment. Expect confusion between neutral vs (positive/negative), especially for rhetorical questions.\n",
    "- `has_url` (contains link): URLs can reduce lexical sentiment signal (more ‚Äúnews-like‚Äù/headline-like), and can include truncated context. Expect more neutral defaults or misclassification when sentiment is implied by context not text.\n",
    "- `has_strong_sentiment` (lexicon like love/hate/best/worst): Strong sentiment words should make classification easier, but models may over-rely on keywords and fail on negation/hedging (e.g., ‚Äúnot the best‚Äù). Expect high confidence wrong predictions in tricky phrasing.\n",
    "- `has_sarcasm_indicators` (phrases like ‚Äúyeah right‚Äù, ‚Äúsure‚Äù, ‚Äútotally‚Äù, ‚Äúobviously‚Äù): These often invert the literal sentiment. Expect large candidate regressions if the model lacks pragmatic understanding of sarcasm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a75f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0\n",
      "transformers: 4.57.1\n",
      "CUDA available: False\n",
      "Python: /opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "# Transformers requires a backend (PyTorch/TensorFlow/Flax). We'll use PyTorch.\n",
    "try:\n",
    "    import torch, transformers, sys\n",
    "    print(\"torch:\", torch.__version__)\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"Python:\", sys.executable)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install PyTorch before proceeding: pip install torch torchvision torchaudio\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ec371",
   "metadata": {},
   "source": [
    "###  Step 4 ‚Äì Run Inference (Two Models)\n",
    "\n",
    "In this step, you'll use two HuggingFace sentiment analysis models to run inference on your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89f69d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf390fe66004d85807cd55724c9a8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Infer: cardiffnlp/twitter-roberta-base-sentiment-latest:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c319c2c5aad450399dd901def9bbba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Infer: LYTinn/finetuning-sentiment-model-tweet-gpt2:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_negation</th>\n",
       "      <th>length_bucket</th>\n",
       "      <th>has_all_caps</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_strong_sentiment</th>\n",
       "      <th>has_sarcasm_indicators</th>\n",
       "      <th>model</th>\n",
       "      <th>pred</th>\n",
       "      <th>conf</th>\n",
       "      <th>ex_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user @user what do these '1/2 naked pics' hav...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>51-100</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.804726</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH: ‚ÄúI had a blue penis while I was this‚Äù [pla...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>51-100</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.866949</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user That's coming, but I think the vic...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>51-100</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.763724</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think I may be finally in with the in crowd ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>51-100</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user Wow,first Hugo Chavez and now Fidel Cast...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>101-200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.416398</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label  emoji_count  \\\n",
       "0  @user @user what do these '1/2 naked pics' hav...   neutral            0   \n",
       "1  OH: ‚ÄúI had a blue penis while I was this‚Äù [pla...   neutral            0   \n",
       "2  @user @user That's coming, but I think the vic...   neutral            0   \n",
       "3  I think I may be finally in with the in crowd ...  positive            0   \n",
       "4  @user Wow,first Hugo Chavez and now Fidel Cast...  negative            0   \n",
       "\n",
       "   has_hashtag  has_mention  has_negation length_bucket  has_all_caps  \\\n",
       "0        False         True          True        51-100         False   \n",
       "1        False        False         False        51-100          True   \n",
       "2        False         True         False        51-100         False   \n",
       "3         True         True         False        51-100         False   \n",
       "4        False         True         False       101-200         False   \n",
       "\n",
       "   has_question  has_url  has_strong_sentiment  has_sarcasm_indicators  \\\n",
       "0          True    False                 False                   False   \n",
       "1         False    False                 False                   False   \n",
       "2         False    False                 False                   False   \n",
       "3         False    False                 False                   False   \n",
       "4         False    False                 False                   False   \n",
       "\n",
       "            model      pred      conf  ex_id  \n",
       "0  baseline_model  negative  0.804726    113  \n",
       "1  baseline_model   neutral  0.866949    363  \n",
       "2  baseline_model   neutral  0.763724    102  \n",
       "3  baseline_model  positive  0.774047    305  \n",
       "4  baseline_model   neutral  0.416398    160  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_pipeline(model_id: str, texts: list[str]):\n",
    "    clf = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model_id,\n",
    "        truncation=True,\n",
    "        max_length=128,     # avoid truncation warnings\n",
    "        framework=\"pt\",\n",
    "        device=-1           # CPU\n",
    "    )\n",
    "    # (Optional) sanity check label mapping for this model\n",
    "    # print(model_id, clf.model.config.id2label)\n",
    "\n",
    "    preds, confs = [], []\n",
    "    for t in tqdm(texts, desc=f\"Infer: {model_id}\"):\n",
    "        out = clf(t)[0]\n",
    "        lbl = HF_LABEL_MAP.get(out[\"label\"], out[\"label\"])\n",
    "        preds.append(lbl)\n",
    "        confs.append(float(out[\"score\"]))\n",
    "    return preds, confs\n",
    "\n",
    "pred_frames = []\n",
    "texts = df[\"text\"].tolist()\n",
    "\n",
    "for model_name, model_id in MODELS.items():\n",
    "    yhat, conf = run_pipeline(model_id, texts)\n",
    "    tmp = df.copy()\n",
    "    tmp[\"model\"] = model_name\n",
    "    tmp[\"pred\"] = yhat\n",
    "    tmp[\"conf\"] = conf\n",
    "    pred_frames.append(tmp)\n",
    "\n",
    "df_long = pd.concat(pred_frames, ignore_index=True)\n",
    "\n",
    "# Add a stable example id so reshaping won't silently drop duplicates\n",
    "df_long[\"ex_id\"] = df_long.groupby([\"text\", \"label\"]).ngroup()\n",
    "\n",
    "df_long.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d189f1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ex_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_negation</th>\n",
       "      <th>length_bucket</th>\n",
       "      <th>has_all_caps</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_strong_sentiment</th>\n",
       "      <th>has_sarcasm_indicators</th>\n",
       "      <th>conf_baseline_model</th>\n",
       "      <th>conf_candidate_model</th>\n",
       "      <th>pred_baseline_model</th>\n",
       "      <th>pred_candidate_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Fatty Kim The Third\" üò≠üò≠üò≠</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0-50</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.486252</td>\n",
       "      <td>0.978987</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Focusing on [alt rightists'] respectability.....</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>51-100</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.573572</td>\n",
       "      <td>0.999728</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\"Kim Fatty the Third\"</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0-50</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.849732</td>\n",
       "      <td>0.937709</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"We have lost everything\": Syrians return to r...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>51-100</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.751955</td>\n",
       "      <td>0.994244</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"who's the most wiped out white boy? Zac Efron...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>51-100</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.561233</td>\n",
       "      <td>0.906541</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ex_id                                               text     label  \\\n",
       "0      0                          \"Fatty Kim The Third\" üò≠üò≠üò≠   neutral   \n",
       "1      1  \"Focusing on [alt rightists'] respectability.....   neutral   \n",
       "2      2                              \"Kim Fatty the Third\"  negative   \n",
       "3      3  \"We have lost everything\": Syrians return to r...   neutral   \n",
       "4      4  \"who's the most wiped out white boy? Zac Efron...   neutral   \n",
       "\n",
       "   emoji_count  has_hashtag  has_mention  has_negation length_bucket  \\\n",
       "0            3        False        False         False          0-50   \n",
       "1            0        False        False         False        51-100   \n",
       "2            0        False        False         False          0-50   \n",
       "3            0         True         True         False        51-100   \n",
       "4            0        False        False         False        51-100   \n",
       "\n",
       "   has_all_caps  has_question  has_url  has_strong_sentiment  \\\n",
       "0         False         False    False                 False   \n",
       "1         False         False    False                 False   \n",
       "2         False         False    False                 False   \n",
       "3          True         False    False                 False   \n",
       "4         False          True    False                 False   \n",
       "\n",
       "   has_sarcasm_indicators  conf_baseline_model  conf_candidate_model  \\\n",
       "0                   False             0.486252              0.978987   \n",
       "1                   False             0.573572              0.999728   \n",
       "2                   False             0.849732              0.937709   \n",
       "3                   False             0.751955              0.994244   \n",
       "4                   False             0.561233              0.906541   \n",
       "\n",
       "  pred_baseline_model pred_candidate_model  \n",
       "0             neutral              neutral  \n",
       "1            negative              neutral  \n",
       "2             neutral              neutral  \n",
       "3            negative             positive  \n",
       "4             neutral             positive  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4.5 ‚Äì Wide-format Table for Model Comparison (Optional but recommended)\n",
    "# One row per tweet, with baseline + candidate predictions in columns\n",
    "# TODO: Replace with your metadata\n",
    "df_wide = df_long.pivot_table(\n",
    "    index=[\n",
    "        \"ex_id\", \"text\", \"label\",\n",
    "        \"emoji_count\", \"has_hashtag\", \"has_mention\", \"has_negation\", \"length_bucket\",\n",
    "        \"has_all_caps\", \"has_question\", \"has_url\", \"has_strong_sentiment\", \"has_sarcasm_indicators\"\n",
    "    ], # Add additional hypothesis-driven slices \n",
    "    columns=\"model\",\n",
    "    values=[\"pred\", \"conf\"],\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names (e.g., pred_baseline_model, conf_candidate_model)\n",
    "df_wide.columns = [\"_\".join([c for c in col if c]).strip(\"_\") for col in df_wide.columns]\n",
    "\n",
    "df_wide.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dedaa9",
   "metadata": {},
   "source": [
    "### Step 5: Compute Metrics (Accuracy + Slice Accuracy + Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b77d0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Edit to work for your slices\n",
    "\n",
    "#compute metrics model-wise\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    return accuracy_score(list(y_true), list(y_pred))\n",
    "\n",
    "# Overall accuracy by model (df_long: one row per (tweet, model))\n",
    "overall = df_long.groupby(\"model\").apply(\n",
    "    lambda g: compute_accuracy(g[\"label\"], g[\"pred\"]),\n",
    "    include_groups=False\n",
    ")\n",
    "\n",
    "# Slice accuracy table (uses df_long masks)\n",
    "slice_table = wandb.Table(columns=[\"slice\", \"model\", \"accuracy\"])\n",
    "slice_metrics = {}\n",
    "\n",
    "for slice_name, mask in get_slices(df_long).items():\n",
    "    slice_metrics[slice_name] = {}\n",
    "    for model_name, g in df_long[mask].groupby(\"model\"):\n",
    "        acc = float(compute_accuracy(g[\"label\"], g[\"pred\"]))\n",
    "        slice_table.add_data(slice_name, model_name, acc)\n",
    "        slice_metrics[slice_name][model_name] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "596b544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression rate: 0.408\n",
      "Improvement rate: 0.108\n",
      "Confident regression rate: 0.382\n"
     ]
    }
   ],
   "source": [
    "# TODO: Edit to work for your slices\n",
    "\n",
    "\n",
    "# Regression-aware evaluation (df_eval: one row per tweet, both model outputs) \n",
    "# A regression is when the candidate gets something wrong that the baseline got right.\n",
    "BASELINE = \"baseline_model\"\n",
    "CANDIDATE = \"candidate_model\"\n",
    "\n",
    "# Ensure ex_id exists (safe even if it already exists)\n",
    "df_long = df_long.copy()\n",
    "if \"ex_id\" not in df_long.columns:\n",
    "    df_long[\"ex_id\"] = df_long.groupby([\"text\", \"label\"]).ngroup()\n",
    "\n",
    "# Build df_eval with metadata carried through\n",
    "df_eval = (\n",
    "    df_long.pivot_table(\n",
    "        index=[\n",
    "            \"ex_id\", \"text\", \"label\",\n",
    "            \"emoji_count\", \"has_hashtag\", \"has_mention\", \"has_negation\", \"length_bucket\",\n",
    "            \"has_all_caps\", \"has_question\", \"has_url\", \"has_strong_sentiment\", \"has_sarcasm_indicators\"\n",
    "        ], # Add additonal hypothesis-driven slices \n",
    "        columns=\"model\",\n",
    "        values=[\"pred\", \"conf\"],\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Flatten column names (pred_baseline_model, conf_candidate_model, etc.)\n",
    "df_eval.columns = [\"_\".join([c for c in col if c]).strip(\"_\") for col in df_eval.columns]\n",
    "\n",
    "# Correctness flags\n",
    "df_eval[\"baseline_correct\"]  = df_eval[f\"pred_{BASELINE}\"] == df_eval[\"label\"]\n",
    "df_eval[\"candidate_correct\"] = df_eval[f\"pred_{CANDIDATE}\"] == df_eval[\"label\"]\n",
    "\n",
    "# Regression / improvement flags\n",
    "df_eval[\"regressed\"]   = df_eval[\"baseline_correct\"] & ~df_eval[\"candidate_correct\"]\n",
    "df_eval[\"improved\"]    = ~df_eval[\"baseline_correct\"] & df_eval[\"candidate_correct\"]\n",
    "df_eval[\"both_wrong\"]  = ~df_eval[\"baseline_correct\"] & ~df_eval[\"candidate_correct\"]\n",
    "df_eval[\"both_correct\"]= df_eval[\"baseline_correct\"] & df_eval[\"candidate_correct\"]\n",
    "\n",
    "# Confidence-conditional regression (candidate is confident AND worse than baseline)\n",
    "df_eval[\"confident_regression\"] = df_eval[\"regressed\"] & (df_eval[f\"conf_{CANDIDATE}\"] >= 0.8)\n",
    "\n",
    "# Global regression metrics\n",
    "regression_rate = float(df_eval[\"regressed\"].mean())\n",
    "improvement_rate = float(df_eval[\"improved\"].mean())\n",
    "conf_reg_rate = float(df_eval[\"confident_regression\"].mean())\n",
    "\n",
    "print(\"Regression rate:\", regression_rate)\n",
    "print(\"Improvement rate:\", improvement_rate)\n",
    "print(\"Confident regression rate:\", conf_reg_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12e21e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Edit to work with your slices\n",
    "\n",
    "# Define slices on df_eval (must use columns that exist in df_eval)\n",
    "def get_slices_eval(df_any):\n",
    "    return {\n",
    "        \"emoji_gt3\": df_any[\"emoji_count\"] > 3,\n",
    "        \"has_negation\": df_any[\"has_negation\"] == True,\n",
    "        \"has_hashtag\": df_any[\"has_hashtag\"] == True,\n",
    "        \"long_tweets\": df_any[\"length_bucket\"].astype(str).isin([\"201-1000\", \"1001+\"]),\n",
    "        \"has_all_caps\": df_any[\"has_all_caps\"] == True, # Added\n",
    "        \"has_question\": df_any[\"has_question\"] == True, # Added\n",
    "        \"has_strong_sentiment\": df_any[\"has_strong_sentiment\"] == True, # Added\n",
    "        \"short_tweets\": df_any[\"length_bucket\"] == \"0-50\", # Added\n",
    "    }\n",
    "\n",
    "# Slice-level regression metrics table\n",
    "reg_table = wandb.Table(columns=[\"slice\", \"metric\", \"value\"])\n",
    "reg_metrics = {}\n",
    "\n",
    "for slice_name, mask in get_slices_eval(df_eval).items():\n",
    "    g = df_eval[mask]\n",
    "    if len(g) == 0:\n",
    "        continue\n",
    "\n",
    "    reg = float(g[\"regressed\"].mean())\n",
    "    imp = float(g[\"improved\"].mean())\n",
    "    conf_reg = float(g[\"confident_regression\"].mean())\n",
    "\n",
    "    reg_table.add_data(slice_name, \"regression_rate\", reg)\n",
    "    reg_table.add_data(slice_name, \"improvement_rate\", imp)\n",
    "    reg_table.add_data(slice_name, \"confident_regression_rate\", conf_reg)\n",
    "\n",
    "    reg_metrics[slice_name] = {\n",
    "        \"regression_rate\": reg,\n",
    "        \"improvement_rate\": imp,\n",
    "        \"conf_reg_rate\": conf_reg\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7843a",
   "metadata": {},
   "source": [
    "### Step 6 ‚Äî #TODO: Log to W&B & Analyse Slices\n",
    "### (Make sure PROJECT/ENTITY/RUN_NAME exist from Step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "425dd4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/joannachang/Documents/26 Spring/ML in production/Labs/cmu-mlip-model-testing-lab/wandb/run-20260204_212425-pgnbbb8r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026/runs/pgnbbb8r' target=\"_blank\">baseline_vs_candidate</a></strong> to <a href='https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026' target=\"_blank\">https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026/runs/pgnbbb8r' target=\"_blank\">https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026/runs/pgnbbb8r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run URL: https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026/runs/pgnbbb8r\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>baseline_model_accuracy</td><td>0.698</td></tr><tr><td>candidate_model_accuracy</td><td>0.398</td></tr><tr><td>confident_regression_rate</td><td>0.382</td></tr><tr><td>improvement_rate</td><td>0.108</td></tr><tr><td>regression_rate</td><td>0.408</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline_vs_candidate</strong> at: <a href='https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026/runs/pgnbbb8r' target=\"_blank\">https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026/runs/pgnbbb8r</a><br> View project at: <a href='https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026' target=\"_blank\">https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026</a><br>Synced 5 W&B file(s), 4 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260204_212425-pgnbbb8r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Log to W&B\n",
    "\n",
    "PROJECT = \"mlip-lab4-slices-2026\"\n",
    "ENTITY = None\n",
    "RUN_NAME = \"baseline_vs_candidate\"\n",
    "run = wandb.init(project=PROJECT, entity=ENTITY, name=RUN_NAME)\n",
    "wandb.log({\"predictions_table\": wandb.Table(dataframe=df_long)})\n",
    "wandb.log({\"slice_metrics\": slice_table})\n",
    "wandb.log({\"regression_metrics\": reg_table})\n",
    "wandb.log({\n",
    "    \"df_eval\": wandb.Table(dataframe=df_eval)\n",
    "})\n",
    "for model_name, acc in overall.items():\n",
    "    wandb.summary[f\"{model_name}_accuracy\"] = float(acc)\n",
    "wandb.summary[\"regression_rate\"] = regression_rate\n",
    "wandb.summary[\"improvement_rate\"] = improvement_rate\n",
    "wandb.summary[\"confident_regression_rate\"] = conf_reg_rate\n",
    "\n",
    "print(\"W&B run URL:\", run.get_url())\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c3b74",
   "metadata": {},
   "source": [
    "### Instructions: Exploring Slice-Based Evaluation in W&B\n",
    "\n",
    "# Purpose\n",
    "In this lab, you are evaluating a candidate sentiment model to decide whether it should replace an existing baseline (production) model.\n",
    "You have already:\n",
    "  - run both models on the same dataset\n",
    "  - logged predictions, confidence scores, and metadata to W&B\n",
    "  - created metadata that allows you to slice the data\n",
    "The most important goal is to understand when and why models behave differently.\n",
    "Overall accuracy alone is often misleading.\n",
    "\n",
    "# What to do in W&B\n",
    "1. Open your W&B run\n",
    "  - Click the project link and open the latest run.\n",
    "2. Explore the predictions table\n",
    "  - Go to the Tables tab and open predictions_table.\n",
    "  - Each row is one tweet √ó one model.\n",
    "3. Create and analyze slices (most important)\n",
    "  - Use filters to create meaningful slices \n",
    "    (e.g., negation, emojis, hashtags, long tweets).\n",
    "  - For each slice:\n",
    "    - Compare baseline vs candidate performance.\n",
    "    - Compare slice accuracy to overall accuracy.\n",
    "    - Inspect a few misclassified examples to identify patterns.\n",
    "4. Visualize slice performance\n",
    "  - Open slice_metrics.\n",
    "  - Create bar charts comparing baseline vs candidate accuracy for at least two slices.\n",
    "5. Discuss your findings with the TA\n",
    "  - Explain why slicing reveals issues that overall accuracy hides.\n",
    "  - Say whether the candidate model should be deployed and why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41f83c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Slice Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emoji_gt3: Tweets with 3+ emojis showed strong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>has_negation: Negation words (not, never, no) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>has_hashtag: Hashtag presence did not signific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>has_all_caps: All-caps words (indicating empha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>has_question: Question tweets showed higher re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>has_strong_sentiment: Tweets with strong senti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>short_tweets: Very short tweets (0-50 chars) l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Slice Notes\n",
       "0  emoji_gt3: Tweets with 3+ emojis showed strong...\n",
       "1  has_negation: Negation words (not, never, no) ...\n",
       "2  has_hashtag: Hashtag presence did not signific...\n",
       "3  has_all_caps: All-caps words (indicating empha...\n",
       "4  has_question: Question tweets showed higher re...\n",
       "5  has_strong_sentiment: Tweets with strong senti...\n",
       "6  short_tweets: Very short tweets (0-50 chars) l..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Students: replace the placeholders below with 1‚Äì2 sentence insights\n",
    "#TODO: Replace this with 1-2 sentence takeaways for each slice.\n",
    "saved_slice_notes = [\n",
    "    \"emoji_gt3: Tweets with 3+ emojis showed strong performance differences between models, suggesting emoji interpretation is a key differentiator.\",\n",
    "    \"has_negation: Negation words (not, never, no) caused the candidate model to struggle significantly more than baseline, indicating poor negation handling.\",\n",
    "    \"has_hashtag: Hashtag presence did not significantly impact model differences, both models performed similarly on this slice.\",\n",
    "    \"has_all_caps: All-caps words (indicating emphasis/shouting) caused confusion in sentiment prediction for both models but more so for the candidate.\",\n",
    "    \"has_question: Question tweets showed higher regression rates, suggesting the candidate model misinterprets interrogative sentiment.\",\n",
    "    \"has_strong_sentiment: Tweets with strong sentiment words (love, hate) had better baseline performance but candidate still regressed significantly.\",\n",
    "    \"short_tweets: Very short tweets (0-50 chars) lack context and both models struggled, but candidate performed worse with higher regression.\"\n",
    "]\n",
    "pd.DataFrame(saved_slice_notes, columns=[\"Slice Notes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ddad4",
   "metadata": {},
   "source": [
    "#### ‚≠êÔ∏èFindings from Slice Visualization ‚Äî 5 key results \n",
    "These come directly from the `slice_metrics` visualization (baseline vs candidate accuracy), with slice sizes shown as `n`.\n",
    "\n",
    "1. **Candidate underperforms baseline on every meaningful slice we tested**, consistent with the overall gap (baseline overall accuracy **0.698** vs candidate **0.398**, Œî = **-0.300**).\n",
    "2. **Biggest failure: `has_question` (n=58)** ‚Äî baseline accuracy **0.655** vs candidate **0.293** (Œî = **-0.362**). Question / rhetorical-question tweets are a major regression risk.\n",
    "3. **Strong sentiment keywords are not ‚Äúeasy mode‚Äù for the candidate: `has_strong_sentiment` (n=20)** ‚Äî baseline **0.950** vs candidate **0.650** (Œî = **-0.300**). The candidate still misses obvious polarity cues.\n",
    "4. **Emphasis / shouting is another large regression: `has_all_caps` (n=102)** ‚Äî baseline **0.667** vs candidate **0.373** (Œî = **-0.294**). The candidate appears to mis-handle intensity markers.\n",
    "5. **Negation remains a consistent weakness: `has_negation` (n=28)** ‚Äî baseline **0.571** vs candidate **0.357** (Œî = **-0.214**). This supports the Step 7 stress test choice (negation-targeted synthetic tweets).\n",
    "\n",
    "> Note: `emoji_gt3` shows extreme values but has **n=1**, so it‚Äôs not reliable evidence by itself‚Äîtreat it as a ‚Äúneeds more data‚Äù slice rather than a conclusion.\n",
    "\n",
    "![W&B slice_metrics visualization (dot plot)](Visualize%20slice%20performance.png)\n",
    "\n",
    "![W&B slice_metrics visualization (bar chart)](Visualize%20slice%20performance%20-bar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa906c",
   "metadata": {},
   "source": [
    "### Step 7 - Targeted stress testing with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396331d",
   "metadata": {},
   "source": [
    "TODO: \n",
    "In this step, you will use a Large Language Model (LLM) to generate test cases that specifically target a weakness you observed during slicing.\n",
    "\n",
    "What to do:\n",
    "1. Choose one slice where you noticed poor performance, regressions, or surprising behavior.\n",
    "2. Write a short hypothesis (1‚Äì2 sentences) explaining why the model might struggle on this slice. Example:\n",
    "‚ÄúThe model struggles with tweets that use slang and sarcasm.‚Äù\n",
    "3. Use an LLM to generate 10 test cases designed to test this hypothesis.\n",
    "These can include:\n",
    "    - subtle or ambiguous cases\n",
    "    - difficult or adversarial cases\n",
    "    - small wording changes that affect sentiment\n",
    "4. Re-run both models on the generated test cases (helper script given below.)\n",
    "5. Briefly describe what you observed to the TA:\n",
    "    - Did the same failures appear again?\n",
    "    - notice any new failure patterns?\n",
    "    - would this affect your confidence in deploying the model?\n",
    "\n",
    "Your input can be in the following format:\n",
    "\n",
    "> Examples:\n",
    "> - @user @user That‚Äôs coming, but I think the victims are going to be Medicaid recipients.\n",
    "> - I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
    "> \n",
    "> Generate more tweets using slangs.\n",
    "\n",
    "Use our provided GPTs to start the task: [llm-based-test-case-generator](https://chatgpt.com/g/g-982cylVn2-llm-based-test-case-generator). If you do not have access to GPTs, use the plain ChatGPT or other LLM providers you have access to instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49623362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Paste your 10 generated tweets here:\n",
    "generated_slice_description = \"Tweets with negation words (not, never, no) that flip sentiment meaning. The candidate model struggles significantly with negation, showing high regression rates when sentiment is reversed by negative words.\"\n",
    "\n",
    "generated_cases = [\n",
    "    \"This movie is not bad at all, actually quite enjoyable!\",\n",
    "    \"I'm never going to say this pizza isn't amazing\",\n",
    "    \"No way this is the worst day ever, it's actually great\",\n",
    "    \"The service was not terrible, but it wasn't good either\",\n",
    "    \"I don't hate this song, it's just not my favorite\",\n",
    "    \"Never thought I'd say this isn't a disappointment\",\n",
    "    \"This restaurant is no good at being bad - it's excellent!\",\n",
    "    \"I'm not unhappy with my purchase, just not thrilled\",\n",
    "    \"The weather isn't not nice today - double negative intended\",\n",
    "    \"Never have I not enjoyed a concert this much\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ce7deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper code to run models on synthetic test cases:\n",
    "\n",
    "def run_on_generated_tests(texts, models=MODELS):\n",
    "    rows = []\n",
    "    for model_name, model_id in models.items():\n",
    "        clf = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model_id,\n",
    "            truncation=True,\n",
    "            framework=\"pt\",\n",
    "            device=-1\n",
    "        )\n",
    "        for t in texts:\n",
    "            out = clf(t)[0]\n",
    "            rows.append({\n",
    "                \"text\": t,\n",
    "                \"model\": model_name,\n",
    "                \"pred\": HF_LABEL_MAP.get(out[\"label\"], out[\"label\"]),\n",
    "                \"conf\": float(out[\"score\"])\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4a55969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>model</th>\n",
       "      <th>pred</th>\n",
       "      <th>conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie is not bad at all, actually quite e...</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.983940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm never going to say this pizza isn't amazing</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.555089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No way this is the worst day ever, it's actual...</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.524262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The service was not terrible, but it wasn't go...</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.844705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't hate this song, it's just not my favorite</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.731894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Never thought I'd say this isn't a disappointment</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.800755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This restaurant is no good at being bad - it's...</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.884516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm not unhappy with my purchase, just not thr...</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.598655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The weather isn't not nice today - double nega...</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.936736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Never have I not enjoyed a concert this much</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.891479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>This movie is not bad at all, actually quite e...</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I'm never going to say this pizza isn't amazing</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.904155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>No way this is the worst day ever, it's actual...</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.999834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The service was not terrible, but it wasn't go...</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.991405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I don't hate this song, it's just not my favorite</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.998578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Never thought I'd say this isn't a disappointment</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>This restaurant is no good at being bad - it's...</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.958428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I'm not unhappy with my purchase, just not thr...</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.999767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The weather isn't not nice today - double nega...</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Never have I not enjoyed a concert this much</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.999980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text            model  \\\n",
       "0   This movie is not bad at all, actually quite e...   baseline_model   \n",
       "1     I'm never going to say this pizza isn't amazing   baseline_model   \n",
       "2   No way this is the worst day ever, it's actual...   baseline_model   \n",
       "3   The service was not terrible, but it wasn't go...   baseline_model   \n",
       "4   I don't hate this song, it's just not my favorite   baseline_model   \n",
       "5   Never thought I'd say this isn't a disappointment   baseline_model   \n",
       "6   This restaurant is no good at being bad - it's...   baseline_model   \n",
       "7   I'm not unhappy with my purchase, just not thr...   baseline_model   \n",
       "8   The weather isn't not nice today - double nega...   baseline_model   \n",
       "9        Never have I not enjoyed a concert this much   baseline_model   \n",
       "10  This movie is not bad at all, actually quite e...  candidate_model   \n",
       "11    I'm never going to say this pizza isn't amazing  candidate_model   \n",
       "12  No way this is the worst day ever, it's actual...  candidate_model   \n",
       "13  The service was not terrible, but it wasn't go...  candidate_model   \n",
       "14  I don't hate this song, it's just not my favorite  candidate_model   \n",
       "15  Never thought I'd say this isn't a disappointment  candidate_model   \n",
       "16  This restaurant is no good at being bad - it's...  candidate_model   \n",
       "17  I'm not unhappy with my purchase, just not thr...  candidate_model   \n",
       "18  The weather isn't not nice today - double nega...  candidate_model   \n",
       "19       Never have I not enjoyed a concert this much  candidate_model   \n",
       "\n",
       "        pred      conf  \n",
       "0   positive  0.983940  \n",
       "1   positive  0.555089  \n",
       "2   positive  0.524262  \n",
       "3   negative  0.844705  \n",
       "4   negative  0.731894  \n",
       "5   negative  0.800755  \n",
       "6   positive  0.884516  \n",
       "7   negative  0.598655  \n",
       "8   negative  0.936736  \n",
       "9   positive  0.891479  \n",
       "10  negative  0.999993  \n",
       "11  negative  0.904155  \n",
       "12  negative  0.999834  \n",
       "13  negative  0.991405  \n",
       "14  negative  0.998578  \n",
       "15  negative  0.999999  \n",
       "16  negative  0.958428  \n",
       "17  negative  0.999767  \n",
       "18  positive  0.999999  \n",
       "19  negative  0.999980  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_df = run_on_generated_tests(generated_cases)\n",
    "generated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a643634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>baseline_conf</th>\n",
       "      <th>candidate_conf</th>\n",
       "      <th>baseline_pred</th>\n",
       "      <th>candidate_pred</th>\n",
       "      <th>intended_label</th>\n",
       "      <th>baseline_correct</th>\n",
       "      <th>candidate_correct</th>\n",
       "      <th>pred_disagree</th>\n",
       "      <th>candidate_high_conf</th>\n",
       "      <th>candidate_high_conf_wrong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm never going to say this pizza isn't amazing</td>\n",
       "      <td>0.555089</td>\n",
       "      <td>0.904155</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Never have I not enjoyed a concert this much</td>\n",
       "      <td>0.891479</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No way this is the worst day ever, it's actual...</td>\n",
       "      <td>0.524262</td>\n",
       "      <td>0.999834</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The weather isn't not nice today - double nega...</td>\n",
       "      <td>0.936736</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This movie is not bad at all, actually quite e...</td>\n",
       "      <td>0.983940</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This restaurant is no good at being bad - it's...</td>\n",
       "      <td>0.884516</td>\n",
       "      <td>0.958428</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't hate this song, it's just not my favorite</td>\n",
       "      <td>0.731894</td>\n",
       "      <td>0.998578</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm not unhappy with my purchase, just not thr...</td>\n",
       "      <td>0.598655</td>\n",
       "      <td>0.999767</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Never thought I'd say this isn't a disappointment</td>\n",
       "      <td>0.800755</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The service was not terrible, but it wasn't go...</td>\n",
       "      <td>0.844705</td>\n",
       "      <td>0.991405</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  baseline_conf  \\\n",
       "1    I'm never going to say this pizza isn't amazing       0.555089   \n",
       "3       Never have I not enjoyed a concert this much       0.891479   \n",
       "5  No way this is the worst day ever, it's actual...       0.524262   \n",
       "7  The weather isn't not nice today - double nega...       0.936736   \n",
       "8  This movie is not bad at all, actually quite e...       0.983940   \n",
       "9  This restaurant is no good at being bad - it's...       0.884516   \n",
       "0  I don't hate this song, it's just not my favorite       0.731894   \n",
       "2  I'm not unhappy with my purchase, just not thr...       0.598655   \n",
       "4  Never thought I'd say this isn't a disappointment       0.800755   \n",
       "6  The service was not terrible, but it wasn't go...       0.844705   \n",
       "\n",
       "   candidate_conf baseline_pred candidate_pred intended_label  \\\n",
       "1        0.904155      positive       negative       positive   \n",
       "3        0.999980      positive       negative        neutral   \n",
       "5        0.999834      positive       negative       positive   \n",
       "7        0.999999      negative       positive        neutral   \n",
       "8        0.999993      positive       negative       positive   \n",
       "9        0.958428      positive       negative       positive   \n",
       "0        0.998578      negative       negative       positive   \n",
       "2        0.999767      negative       negative       positive   \n",
       "4        0.999999      negative       negative        neutral   \n",
       "6        0.991405      negative       negative       positive   \n",
       "\n",
       "   baseline_correct  candidate_correct  pred_disagree  candidate_high_conf  \\\n",
       "1              True              False           True                 True   \n",
       "3             False              False           True                 True   \n",
       "5              True              False           True                 True   \n",
       "7             False              False           True                 True   \n",
       "8              True              False           True                 True   \n",
       "9              True              False           True                 True   \n",
       "0             False              False          False                 True   \n",
       "2             False              False          False                 True   \n",
       "4             False              False          False                 True   \n",
       "6             False              False          False                 True   \n",
       "\n",
       "   candidate_high_conf_wrong  \n",
       "1                       True  \n",
       "3                       True  \n",
       "5                       True  \n",
       "7                       True  \n",
       "8                       True  \n",
       "9                       True  \n",
       "0                       True  \n",
       "2                       True  \n",
       "4                       True  \n",
       "6                       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'n_cases': 10,\n",
       " 'baseline_accuracy_on_synthetic': 0.4,\n",
       " 'candidate_accuracy_on_synthetic': 0.0,\n",
       " 'disagreement_rate': 0.6,\n",
       " 'candidate_high_conf_wrong_count': 10,\n",
       " 'candidate_high_conf_wrong_rate': 1.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Step 7 analysis: present & compare the two models on the 10 synthetic negation cases\n",
    "\n",
    "# 1) Make a wide table: one row per synthetic tweet, baseline + candidate side-by-side\n",
    "generated_wide = (\n",
    "    generated_df\n",
    "    .pivot_table(index=\"text\", columns=\"model\", values=[\"pred\", \"conf\"], aggfunc=\"first\")\n",
    "    .reset_index()\n",
    ")\n",
    "generated_wide.columns = [\"_\".join([c for c in col if c]).strip(\"_\") for col in generated_wide.columns]\n",
    "generated_wide = generated_wide.rename(columns={\n",
    "    f\"pred_{BASELINE}\": \"baseline_pred\",\n",
    "    f\"conf_{BASELINE}\": \"baseline_conf\",\n",
    "    f\"pred_{CANDIDATE}\": \"candidate_pred\",\n",
    "    f\"conf_{CANDIDATE}\": \"candidate_conf\",\n",
    "})\n",
    "\n",
    "# 2) Optional (recommended): add intended label for each synthetic case, so you can compute accuracy\n",
    "# Edit these if you intended different sentiments for your generated cases.\n",
    "intended_label = [\n",
    "    \"positive\",  # not bad ... enjoyable\n",
    "    \"positive\",  # never ... isn't amazing (double negation)\n",
    "    \"positive\",  # not worst ... actually great\n",
    "    \"neutral\",   # not terrible, but not good either (mixed)\n",
    "    \"neutral\",   # don't hate ... not my favorite (mild negative / neutral)\n",
    "    \"positive\",  # isn't a disappointment (double negation)\n",
    "    \"positive\",  # no good at being bad ... excellent\n",
    "    \"neutral\",   # not unhappy ... not thrilled\n",
    "    \"positive\",  # isn't not nice (double negation)\n",
    "    \"positive\",  # never have I not enjoyed (double negation)\n",
    " ]\n",
    "\n",
    "generated_wide[\"intended_label\"] = intended_label\n",
    "generated_wide[\"baseline_correct\"] = generated_wide[\"baseline_pred\"] == generated_wide[\"intended_label\"]\n",
    "generated_wide[\"candidate_correct\"] = generated_wide[\"candidate_pred\"] == generated_wide[\"intended_label\"]\n",
    "generated_wide[\"pred_disagree\"] = generated_wide[\"baseline_pred\"] != generated_wide[\"candidate_pred\"]\n",
    "generated_wide[\"candidate_high_conf\"] = generated_wide[\"candidate_conf\"] >= 0.9\n",
    "generated_wide[\"candidate_high_conf_wrong\"] = generated_wide[\"candidate_high_conf\"] & (~generated_wide[\"candidate_correct\"])\n",
    "\n",
    "# 3) Summary numbers to report to the TA\n",
    "stress_summary = {\n",
    "    \"n_cases\": int(generated_wide.shape[0]),\n",
    "    \"baseline_accuracy_on_synthetic\": float(generated_wide[\"baseline_correct\"].mean()),\n",
    "    \"candidate_accuracy_on_synthetic\": float(generated_wide[\"candidate_correct\"].mean()),\n",
    "    \"disagreement_rate\": float(generated_wide[\"pred_disagree\"].mean()),\n",
    "    \"candidate_high_conf_wrong_count\": int(generated_wide[\"candidate_high_conf_wrong\"].sum()),\n",
    "    \"candidate_high_conf_wrong_rate\": float(generated_wide[\"candidate_high_conf_wrong\"].mean()),\n",
    "}\n",
    "\n",
    "display(generated_wide.sort_values([\"candidate_high_conf_wrong\", \"pred_disagree\"], ascending=False))\n",
    "stress_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cc034",
   "metadata": {},
   "source": [
    "### Step 7 ‚Äî Stress test (Negation) write-up for TA\n",
    "\n",
    "**Chosen slice:** `has_negation` (tweets with negation words like *not / never / no*).\n",
    "\n",
    "**Hypothesis (1‚Äì2 sentences):** Negation flips sentiment polarity (e.g., ‚Äúnot bad‚Äù ‚Üí positive), and models that over-rely on keywords can fail badly‚Äîoften with high confidence‚Äîwhen the true sentiment depends on scope/double-negatives.\n",
    "\n",
    "**How I generated tests:** I used an LLM to create 10 short tweets that contain single negation, mixed sentiment (\"not terrible, but not good\"), and double negation (\"isn't not nice\"). These are meant to probe whether the model correctly handles negation scope and polarity flips.\n",
    "\n",
    "**How I present the comparison:**\n",
    "- I view the side-by-side table `generated_wide` (and the W&B table `synthetic_tests_wide`) where each row is one synthetic tweet with both models‚Äô `pred` and `conf`.\n",
    "- I sort/filter on `candidate_high_conf_wrong` and `pred_disagree` to highlight the most deployment-relevant failures (high-confidence wrong predictions + baseline/candidate disagreements).\n",
    "\n",
    "**Key observations (from `stress_summary`, using my intended labels for these synthetic cases):**\n",
    "- $n=10$ synthetic negation cases.\n",
    "- Baseline accuracy on these cases: **0.40**.\n",
    "- Candidate accuracy on these cases: **0.00**.\n",
    "- Disagreement rate (baseline vs candidate): **0.60**.\n",
    "- Candidate high-confidence wrong predictions: **10/10 (100%)** (very risky failure mode).\n",
    "\n",
    "**Conclusion / impact on deployment decision:** This stress test reproduces and amplifies the slice-based finding that negation is a major weakness‚Äîespecially for the candidate model. The combination of *worse performance* and *high-confidence errors* on negation cases substantially lowers confidence in deploying the candidate model without targeted remediation and additional testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5752e207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/joannachang/Documents/26 Spring/ML in production/Labs/cmu-mlip-model-testing-lab/wandb/run-20260205_095453-4l5hxhe7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026/runs/4l5hxhe7' target=\"_blank\">stress_test_negation</a></strong> to <a href='https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026' target=\"_blank\">https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026/runs/4l5hxhe7' target=\"_blank\">https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026/runs/4l5hxhe7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stress_test_negation</strong> at: <a href='https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026/runs/4l5hxhe7' target=\"_blank\">https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026/runs/4l5hxhe7</a><br> View project at: <a href='https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026' target=\"_blank\">https://wandb.ai/joannac2-carnegie-mellon-university/mlip-lab4-slices-2026</a><br>Synced 4 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260205_095453-4l5hxhe7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OPTIONAL: Log synthetic test cases to W&B (long + wide + summary metrics)\n",
    "run = wandb.init(project=PROJECT, entity=ENTITY, name=\"stress_test_negation\", reinit=True)\n",
    "wandb.log({\n",
    "    \"synthetic_tests_long\": wandb.Table(dataframe=generated_df),\n",
    "    \"synthetic_tests_wide\": wandb.Table(dataframe=generated_wide),\n",
    "    \"stress_test_summary\": stress_summary,\n",
    "})\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
