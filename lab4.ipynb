{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fcf777d",
   "metadata": {},
   "source": [
    "You are evaluating a candidate sentiment model to replace a production baseline. Your goal is to determine whether this model should ship.\n",
    "\n",
    "“Ship” means: we would choose the candidate model over the baseline for deployment based on the evidence you collect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfc1c7",
   "metadata": {},
   "source": [
    "### Step 1 - Install the required dependencies, set up W&B and make sure the python version is 3.10 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1012c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb datasets transformers evaluate tqdm emoji regex pandas pyarrow scikit-learn nbformat torch\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01639808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262b991",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and config:\n",
    "import re, regex, emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# WANDB CONFIG\n",
    "PROJECT = \"mlip-lab4-slices-2026\"\n",
    "ENTITY = None\n",
    "RUN_NAME = \"baseline_vs_candidate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare\n",
    "MODELS = {\n",
    "    \"baseline_model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    \"candidate_model\":    \"LYTinn/finetuning-sentiment-model-tweet-gpt2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label normalization for tweet_eval (0/1/2 -> string labels)\n",
    "ID2LABEL = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "# Many HF sentiment models output labels like LABEL_0 / LABEL_1 / LABEL_2\n",
    "HF_LABEL_MAP = {\"LABEL_0\": \"negative\", \"LABEL_1\": \"neutral\", \"LABEL_2\": \"positive\"}\n",
    "\n",
    "USE_HF_DATASET = True  # set False to use tweets.csv fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289dcac3",
   "metadata": {},
   "source": [
    "### Step 2 - Load a dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HF_DATASET:\n",
    "    ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "    df = pd.DataFrame(ds[\"test\"]).head(500).copy()\n",
    "    df[\"label\"] = df[\"label\"].map(ID2LABEL)\n",
    "else:\n",
    "    df = pd.read_csv(\"tweets.csv\")\n",
    "    # Ensure it has 'text' and 'label' columns\n",
    "    df = df.rename(columns={c: c.strip() for c in df.columns})\n",
    "    assert {\"text\",\"label\"}.issubset(df.columns), \"tweets.csv must include text,label\"\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.lower()\n",
    "\n",
    "df = df[[\"text\",\"label\"]].dropna().reset_index(drop=True)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0546f1",
   "metadata": {},
   "source": [
    "### Step 3 - Define Failure-Relevant Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089146d4",
   "metadata": {},
   "source": [
    "#TODO:\n",
    "In this step, you will create **at least 5** metadata columns that help you slice and analyze model behavior in Weights & Biases (W&B).\n",
    "These metadata columns should **capture meaningful properties of the data or model behavior that may influence performance**. You can define them using:\n",
    "\n",
    "1. Value matching (e.g., tweets containing hashtags or mentions)\n",
    "2. Regex patterns (e.g., negation words, strong sentiment terms like love or hate)\n",
    "3. Heuristics (e.g., emoji count, all-caps text, tweet length buckets)\n",
    "\n",
    "Each metadata column should correspond to a potential hypothesis about when or why a model might succeed or fail.\n",
    "These columns will be propagated through inference and included in the final predictions_table logged to W&B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62499536",
   "metadata": {},
   "source": [
    "After inference, your W&B table (df_long) will contain:\n",
    "- The original tweet text\n",
    "- Ground-truth sentiment labels\n",
    "- Model predictions and confidence scores\n",
    "- All metadata columns you defined for slicing\n",
    "\n",
    "You will use these metadata fields in the W&B UI (via the ➕ Filter option) to:\n",
    "- Create slices of the data\n",
    "- Compare model behavior across slices\n",
    "- Identify patterns, weaknesses, or regressions that are not visible in overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd5775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 – Add slicing metadata (text-only)\n",
    "\n",
    "# TODO: add your own hypothesis-driven metadata here. \n",
    "# Here are examples of the kinds of metadata columns you can add & analyse.\n",
    "# Categories you can explore are: Linguistic, Emotional/semantic, Model-behavioral. Do not reuse the ones given below.\n",
    "def count_emojis(text: str) -> int:\n",
    "    return sum(ch in emoji.EMOJI_DATA for ch in str(text))\n",
    "\n",
    "df[\"emoji_count\"] = df[\"text\"].apply(count_emojis).astype(int)\n",
    "df[\"has_hashtag\"] = df[\"text\"].str.contains(r\"#\\w+\", regex=True)\n",
    "df[\"has_mention\"] = df[\"text\"].str.contains(r\"@\\w+\", regex=True)\n",
    "df[\"has_negation\"] = df[\"text\"].str.contains(r\"\\b(not|never|no)\\b\", regex=True)\n",
    "df[\"length_bucket\"] = pd.cut(\n",
    "    df[\"text\"].str.len(),\n",
    "    bins=[0, 50, 100, 200, 1000, 10_000],\n",
    "    labels=[\"0-50\", \"51-100\", \"101-200\", \"201-1000\", \"1001+\"],\n",
    "    include_lowest=True\n",
    ").astype(str)\n",
    "\n",
    "# Example slice definitions (you'll create more later)\n",
    "def get_slices(df_any: pd.DataFrame):\n",
    "    return {\n",
    "        \"emoji_gt3\": df_any[\"emoji_count\"] > 3,\n",
    "        \"has_negation\": df_any[\"has_negation\"] == True,\n",
    "        \"has_hashtag\": df_any[\"has_hashtag\"] == True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a75f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers requires a backend (PyTorch/TensorFlow/Flax). We'll use PyTorch.\n",
    "try:\n",
    "    import torch, transformers, sys\n",
    "    print(\"torch:\", torch.__version__)\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"Python:\", sys.executable)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install PyTorch before proceeding: pip install torch torchvision torchaudio\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ec371",
   "metadata": {},
   "source": [
    "###  Step 4 – Run Inference (Two Models)\n",
    "\n",
    "In this step, you'll use two HuggingFace sentiment analysis models to run inference on your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f69d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_pipeline(model_id: str, texts: list[str]):\n",
    "    clf = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model_id,\n",
    "        truncation=True,\n",
    "        max_length=128,     # avoid truncation warnings\n",
    "        framework=\"pt\",\n",
    "        device=-1           # CPU\n",
    "    )\n",
    "    # (Optional) sanity check label mapping for this model\n",
    "    # print(model_id, clf.model.config.id2label)\n",
    "\n",
    "    preds, confs = [], []\n",
    "    for t in tqdm(texts, desc=f\"Infer: {model_id}\"):\n",
    "        out = clf(t)[0]\n",
    "        lbl = HF_LABEL_MAP.get(out[\"label\"], out[\"label\"])\n",
    "        preds.append(lbl)\n",
    "        confs.append(float(out[\"score\"]))\n",
    "    return preds, confs\n",
    "\n",
    "pred_frames = []\n",
    "texts = df[\"text\"].tolist()\n",
    "\n",
    "for model_name, model_id in MODELS.items():\n",
    "    yhat, conf = run_pipeline(model_id, texts)\n",
    "    tmp = df.copy()\n",
    "    tmp[\"model\"] = model_name\n",
    "    tmp[\"pred\"] = yhat\n",
    "    tmp[\"conf\"] = conf\n",
    "    pred_frames.append(tmp)\n",
    "\n",
    "df_long = pd.concat(pred_frames, ignore_index=True)\n",
    "\n",
    "# Add a stable example id so reshaping won't silently drop duplicates\n",
    "df_long[\"ex_id\"] = df_long.groupby([\"text\", \"label\"]).ngroup()\n",
    "\n",
    "df_long.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.5 – Wide-format Table for Model Comparison (Optional but recommended)\n",
    "# One row per tweet, with baseline + candidate predictions in columns\n",
    "# TODO: Replace with your metadata\n",
    "df_wide = df_long.pivot_table(\n",
    "    index=[\n",
    "        \"ex_id\", \"text\", \"label\",\n",
    "        \"emoji_count\", \"has_hashtag\", \"has_mention\", \"has_negation\", \"length_bucket\"\n",
    "    ],\n",
    "    columns=\"model\",\n",
    "    values=[\"pred\", \"conf\"],\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names (e.g., pred_baseline_model, conf_candidate_model)\n",
    "df_wide.columns = [\"_\".join([c for c in col if c]).strip(\"_\") for col in df_wide.columns]\n",
    "\n",
    "df_wide.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dedaa9",
   "metadata": {},
   "source": [
    "### Step 5: Compute Metrics (Accuracy + Slice Accuracy + Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Edit to work for your slices\n",
    "\n",
    "#compute metrics model-wise\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    return accuracy_score(list(y_true), list(y_pred))\n",
    "\n",
    "# Overall accuracy by model (df_long: one row per (tweet, model))\n",
    "overall = df_long.groupby(\"model\").apply(\n",
    "    lambda g: compute_accuracy(g[\"label\"], g[\"pred\"]),\n",
    "    include_groups=False\n",
    ")\n",
    "\n",
    "# Slice accuracy table (uses df_long masks)\n",
    "slice_table = wandb.Table(columns=[\"slice\", \"model\", \"accuracy\"])\n",
    "slice_metrics = {}\n",
    "\n",
    "for slice_name, mask in get_slices(df_long).items():\n",
    "    slice_metrics[slice_name] = {}\n",
    "    for model_name, g in df_long[mask].groupby(\"model\"):\n",
    "        acc = float(compute_accuracy(g[\"label\"], g[\"pred\"]))\n",
    "        slice_table.add_data(slice_name, model_name, acc)\n",
    "        slice_metrics[slice_name][model_name] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Edit to work for your slices\n",
    "\n",
    "\n",
    "# Regression-aware evaluation (df_eval: one row per tweet, both model outputs) \n",
    "# A regression is when the candidate gets something wrong that the baseline got right.\n",
    "BASELINE = \"baseline_model\"\n",
    "CANDIDATE = \"candidate_model\"\n",
    "\n",
    "# Ensure ex_id exists (safe even if it already exists)\n",
    "df_long = df_long.copy()\n",
    "if \"ex_id\" not in df_long.columns:\n",
    "    df_long[\"ex_id\"] = df_long.groupby([\"text\", \"label\"]).ngroup()\n",
    "\n",
    "# Build df_eval with metadata carried through\n",
    "df_eval = (\n",
    "    df_long.pivot_table(\n",
    "        index=[\n",
    "            \"ex_id\", \"text\", \"label\",\n",
    "            \"emoji_count\", \"has_hashtag\", \"has_mention\", \"has_negation\", \"length_bucket\"\n",
    "        ],\n",
    "        columns=\"model\",\n",
    "        values=[\"pred\", \"conf\"],\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Flatten column names (pred_baseline_model, conf_candidate_model, etc.)\n",
    "df_eval.columns = [\"_\".join([c for c in col if c]).strip(\"_\") for col in df_eval.columns]\n",
    "\n",
    "# Correctness flags\n",
    "df_eval[\"baseline_correct\"]  = df_eval[f\"pred_{BASELINE}\"] == df_eval[\"label\"]\n",
    "df_eval[\"candidate_correct\"] = df_eval[f\"pred_{CANDIDATE}\"] == df_eval[\"label\"]\n",
    "\n",
    "# Regression / improvement flags\n",
    "df_eval[\"regressed\"]   = df_eval[\"baseline_correct\"] & ~df_eval[\"candidate_correct\"]\n",
    "df_eval[\"improved\"]    = ~df_eval[\"baseline_correct\"] & df_eval[\"candidate_correct\"]\n",
    "df_eval[\"both_wrong\"]  = ~df_eval[\"baseline_correct\"] & ~df_eval[\"candidate_correct\"]\n",
    "df_eval[\"both_correct\"]= df_eval[\"baseline_correct\"] & df_eval[\"candidate_correct\"]\n",
    "\n",
    "# Confidence-conditional regression (candidate is confident AND worse than baseline)\n",
    "df_eval[\"confident_regression\"] = df_eval[\"regressed\"] & (df_eval[f\"conf_{CANDIDATE}\"] >= 0.8)\n",
    "\n",
    "# Global regression metrics\n",
    "regression_rate = float(df_eval[\"regressed\"].mean())\n",
    "improvement_rate = float(df_eval[\"improved\"].mean())\n",
    "conf_reg_rate = float(df_eval[\"confident_regression\"].mean())\n",
    "\n",
    "print(\"Regression rate:\", regression_rate)\n",
    "print(\"Improvement rate:\", improvement_rate)\n",
    "print(\"Confident regression rate:\", conf_reg_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e21e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Edit to work with your slices\n",
    "\n",
    "# Define slices on df_eval (must use columns that exist in df_eval)\n",
    "def get_slices_eval(df_any):\n",
    "    return {\n",
    "        \"emoji_gt3\": df_any[\"emoji_count\"] > 3,\n",
    "        \"has_negation\": df_any[\"has_negation\"] == True,\n",
    "        \"has_hashtag\": df_any[\"has_hashtag\"] == True,\n",
    "        \"long_tweets\": df_any[\"length_bucket\"].astype(str).isin([\"(200, 1000]\", \"(1000, 10000]\"]),\n",
    "    }\n",
    "\n",
    "# Slice-level regression metrics table\n",
    "reg_table = wandb.Table(columns=[\"slice\", \"metric\", \"value\"])\n",
    "reg_metrics = {}\n",
    "\n",
    "for slice_name, mask in get_slices_eval(df_eval).items():\n",
    "    g = df_eval[mask]\n",
    "    if len(g) == 0:\n",
    "        continue\n",
    "\n",
    "    reg = float(g[\"regressed\"].mean())\n",
    "    imp = float(g[\"improved\"].mean())\n",
    "    conf_reg = float(g[\"confident_regression\"].mean())\n",
    "\n",
    "    reg_table.add_data(slice_name, \"regression_rate\", reg)\n",
    "    reg_table.add_data(slice_name, \"improvement_rate\", imp)\n",
    "    reg_table.add_data(slice_name, \"confident_regression_rate\", conf_reg)\n",
    "\n",
    "    reg_metrics[slice_name] = {\n",
    "        \"regression_rate\": reg,\n",
    "        \"improvement_rate\": imp,\n",
    "        \"conf_reg_rate\": conf_reg\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7843a",
   "metadata": {},
   "source": [
    "# Step 6 — #TODO: Log to W&B & Analyse Slices\n",
    "# (Make sure PROJECT/ENTITY/RUN_NAME exist from Step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425dd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Log to W&B\n",
    "\n",
    "PROJECT = \"mlip-lab4-slices-2026\"\n",
    "ENTITY = None\n",
    "RUN_NAME = \"baseline_vs_candidate\"\n",
    "run = wandb.init(project=PROJECT, entity=ENTITY, name=RUN_NAME)\n",
    "wandb.log({\"predictions_table\": wandb.Table(dataframe=df_long)})\n",
    "wandb.log({\"slice_metrics\": slice_table})\n",
    "wandb.log({\"regression_metrics\": reg_table})\n",
    "wandb.log({\n",
    "    \"df_eval\": wandb.Table(dataframe=df_eval)\n",
    "})\n",
    "for model_name, acc in overall.items():\n",
    "    wandb.summary[f\"{model_name}_accuracy\"] = float(acc)\n",
    "wandb.summary[\"regression_rate\"] = regression_rate\n",
    "wandb.summary[\"improvement_rate\"] = improvement_rate\n",
    "wandb.summary[\"confident_regression_rate\"] = conf_reg_rate\n",
    "\n",
    "print(\"W&B run URL:\", run.get_url())\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c3b74",
   "metadata": {},
   "source": [
    "### Instructions: Exploring Slice-Based Evaluation in W&B\n",
    "\n",
    "# Purpose\n",
    "In this lab, you are evaluating a candidate sentiment model to decide whether it should replace an existing baseline (production) model.\n",
    "You have already:\n",
    "  - run both models on the same dataset\n",
    "  - logged predictions, confidence scores, and metadata to W&B\n",
    "  - created metadata that allows you to slice the data\n",
    "The most important goal is to understand when and why models behave differently.\n",
    "Overall accuracy alone is often misleading.\n",
    "\n",
    "# What to do in W&B\n",
    "1. Open your W&B run\n",
    "  - Click the project link and open the latest run.\n",
    "2. Explore the predictions table\n",
    "  - Go to the Tables tab and open predictions_table.\n",
    "  - Each row is one tweet × one model.\n",
    "3. Create and analyze slices (most important)\n",
    "  - Use filters to create meaningful slices \n",
    "    (e.g., negation, emojis, hashtags, long tweets).\n",
    "  - For each slice:\n",
    "    - Compare baseline vs candidate performance.\n",
    "    - Compare slice accuracy to overall accuracy.\n",
    "    - Inspect a few misclassified examples to identify patterns.\n",
    "4. Visualize slice performance\n",
    "  - Open slice_metrics.\n",
    "  - Create bar charts comparing baseline vs candidate accuracy for at least two slices.\n",
    "5. Discuss your findings with the TA\n",
    "  - Explain why slicing reveals issues that overall accuracy hides.\n",
    "  - Say whether the candidate model should be deployed and why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f83c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students: replace the placeholders below with 1–2 sentence insights\n",
    "#TODO: Replace this with 1-2 sentence takeaways for each slice.\n",
    "saved_slice_notes = [\"...\"]\n",
    "pd.DataFrame(saved_slice_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa906c",
   "metadata": {},
   "source": [
    "### Step 7 - Targeted stress testing with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396331d",
   "metadata": {},
   "source": [
    "TODO: \n",
    "In this step, you will use a Large Language Model (LLM) to generate test cases that specifically target a weakness you observed during slicing.\n",
    "\n",
    "What to do:\n",
    "1. Choose one slice where you noticed poor performance, regressions, or surprising behavior.\n",
    "2. Write a short hypothesis (1–2 sentences) explaining why the model might struggle on this slice. Example:\n",
    "“The model struggles with tweets that use slang and sarcasm.”\n",
    "3. Use an LLM to generate 10 test cases designed to test this hypothesis.\n",
    "These can include:\n",
    "    - subtle or ambiguous cases\n",
    "    - difficult or adversarial cases\n",
    "    - small wording changes that affect sentiment\n",
    "4. Re-run both models on the generated test cases (helper script given below.)\n",
    "5. Briefly describe what you observed to the TA:\n",
    "    - Did the same failures appear again?\n",
    "    - notice any new failure patterns?\n",
    "    - would this affect your confidence in deploying the model?\n",
    "\n",
    "Your input can be in the following format:\n",
    "\n",
    "> Examples:\n",
    "> - @user @user That’s coming, but I think the victims are going to be Medicaid recipients.\n",
    "> - I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
    "> \n",
    "> Generate more tweets using slangs.\n",
    "\n",
    "Use our provided GPTs to start the task: [llm-based-test-case-generator](https://chatgpt.com/g/g-982cylVn2-llm-based-test-case-generator). If you do not have access to GPTs, use the plain ChatGPT or other LLM providers you have access to instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49623362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Paste your 10 generated tweets here:\n",
    "generated_slice_description = \"\"\n",
    "\n",
    "generated_cases = [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper code to run models on synthetic test cases:\n",
    "\n",
    "def run_on_generated_tests(texts, models=MODELS):\n",
    "    rows = []\n",
    "    for model_name, model_id in models.items():\n",
    "        clf = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model_id,\n",
    "            truncation=True,\n",
    "            framework=\"pt\",\n",
    "            device=-1\n",
    "        )\n",
    "        for t in texts:\n",
    "            out = clf(t)[0]\n",
    "            rows.append({\n",
    "                \"text\": t,\n",
    "                \"model\": model_name,\n",
    "                \"pred\": HF_LABEL_MAP.get(out[\"label\"], out[\"label\"]),\n",
    "                \"conf\": float(out[\"score\"])\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a55969",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df = run_on_generated_tests(generated_cases)\n",
    "generated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5752e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Log synthetic test cases to W&B\n",
    "wandb.log({\n",
    "    \"synthetic_tests\": wandb.Table(dataframe=generated_df)\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aieng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
